Welcome to Prompt Science, where we explore how artificial intelligence is transforming the way science itself is done.  
This week, we travel across the expanding frontiers of AI in scientific research, from models that solve astrophysics olympiad problems at a gold-medal level, to generative molecular models that bridge ultrafast time scales in chemistry, to brain-mapping transformers that uncover fine regions of the mouse cortex, and to a cautionary story about how AI-generated toxins can evade safety checks. Each of these stories shows not just progress, but also the responsibilities that come with building machines capable of reasoning and creating.  

Let’s begin with one of the most remarkable demonstrations of scientific reasoning by machines.  
A group of researchers led by Lucas Carrit Delgado Pinheiro and collaborators from the Ohio State University and the Australian National University published an arXiv preprint titled “Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy and Astrophysics.” They evaluated state-of-the-art large language models on past IOAA exams — a contest designed to test high-school students on real astrophysics problems that require deriving formulas, interpreting images, and combining multiple physical laws under time pressure.  
The results are astonishing: models like GPT-4-Turbo and Gemini 1.5 Pro reached scores consistent with human gold-medal winners, especially on the theory sections. The models struggled more with data-analysis and geometry tasks that required direct numerical calculation or image-based measurement, but their qualitative reasoning was often correct.  
Technically, the study used a mixture of structured prompting and chain-of-thought generation, guiding the models through multi-step reasoning while preventing the use of external tools. The researchers analyzed the responses for conceptual accuracy, numerical precision, and reasoning depth, finding that large models can perform symbolic reasoning similar to human students when the problem statement is well specified.  
This work is significant because it quantifies, for the first time, how close AI has come to performing textbook-level physics derivations, not as memorization but as reasoning. It raises questions about the role of models as teaching assistants, and perhaps even as co-solvers in future scientific competitions.  

From the cosmos to the molecular world, another arXiv study this week pushes the limits of generative modeling in physics.  
Juan Viguera Díez, Mathias Schreiner, and Simon Olsson introduced a framework called Transferable Generative Models for Molecular Dynamics. Traditional molecular dynamics simulations operate at femtosecond time steps — a millionth of a billionth of a second — meaning that exploring nanoseconds of motion requires millions of steps. Their new model learns the probability distribution of atomic transitions and can generate realistic trajectories that effectively “jump” from femtoseconds to nanoseconds while preserving the correct thermodynamic statistics.  
Under the hood, the system combines conditional normalizing flows with energy-based regularization. Each molecular configuration is represented in a latent space, and the generative model learns transition kernels that approximate the Boltzmann distribution. During inference, it samples future states directly, bypassing explicit integration of Newton’s equations.  
What makes this powerful is the model’s ability to generalize: trained on small molecules and peptides, it successfully predicts larger and chemically different systems, showing that local structural motifs generalize across molecules. This breakthrough effectively reduces the computational cost of exploring molecular conformations and could dramatically accelerate discovery in drug design and materials science. It also provides an elegant bridge between statistical physics and machine learning, treating trajectories as samples from a generative prior constrained by physical laws.  

Shifting to neuroscience, another paper from the Allen Institute for Brain Science and the University of California, San Francisco presents a deep-learning model named CellTransformer. This transformer-based architecture uses massive spatial-transcriptomics datasets — millions of cells with both genetic and spatial information — to identify previously unannotated microregions in the mouse brain.  
The model combines convolutional encoders that capture local gene-expression patterns with global self-attention layers that infer spatial relationships across tissue slices. Trained without explicit human labels, it performs unsupervised segmentation, effectively reconstructing the brain’s architecture purely from molecular signatures.  
CellTransformer identified more than thirteen hundred distinct regions, including several small nuclei not described in the Allen Brain Atlas. The model uses a contrastive learning objective: embeddings from nearby cells are encouraged to be similar, while distant ones diverge, allowing fine-grained boundary detection. This approach provides neuroscientists with a data-driven way to define what constitutes a functional region, and could reveal how gene-expression patterns relate to behavior, disease, and development.  

But alongside these achievements, a darker story emerged that highlights the urgency of biosecurity in the age of generative models.  
Multiple reports — including findings by a Microsoft research team — showed that AI systems can generate synthetic toxin sequences that evade standard safety screens used by commercial DNA-synthesis companies. By training protein-language models on toxin-related datasets and then performing adversarial sampling, the researchers generated thousands of novel but functional variants of known dangerous proteins such as ricin A chain and botulinum neurotoxin.  
When those sequences were tested against vendor-grade screening software, a fraction bypassed detection because the filters rely on direct sequence similarity. The AI-generated variants altered non-critical residues, enough to avoid matching known toxins while preserving structural motifs responsible for toxicity.  
Microsoft coordinated a responsible disclosure process and developed improved filters, but the experiment exposed a real vulnerability: safety tools built for static databases are now facing adaptive adversarial generation. Technically, it’s the same mechanism that enables creative protein design — language-model-style paraphrasing in amino-acid space — but here applied to dangerous ends.  
This finding underscores the need for adversarially trained biosecurity filters, access control for generative biochemical models, and stronger norms for publication and dataset sharing. The same models that can help design new therapeutics can also, if misused, design harm.  

Taken together, these stories illustrate the dual edge of scientific progress in the age of AI.  
We now have language models capable of solving physics problems that once took human intuition, generative dynamics engines that compress months of simulation into seconds, neural atlases that redraw the brain from molecular data, and sequence generators powerful enough to challenge our biosecurity infrastructure. Each is a glimpse of the expanding frontier where computation meets creativity — and where ethics and governance must keep pace with innovation.  

That’s all for this episode of Prompt Science. You can read transcripts, check references, and contribute improvements to the script at promptscience.org. Join the conversation on X at at-prompt-underscore-science, and if you have corrections or ideas for future topics, open a pull request on the repository. Until next time, keep your reasoning rigorous and your prompts transparent.
